import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Dense, Dropout, Conv2D, SeparableConv2D, MaxPooling2D, 
                                      BatchNormalization, GlobalAveragePooling2D)
from tensorflow.keras.optimizers.legacy import Adam
from tensorflow.keras.regularizers import l2

# Training parameters
EPOCHS = args.epochs or 120
LEARNING_RATE = args.learning_rate or 0.0005
BATCH_SIZE = args.batch_size or 32
ENSURE_DETERMINISM = args.ensure_determinism

# Data augmentation (keep identical to maintain accuracy)
def augment_image(image, label):
    angle = tf.random.uniform([], -0.17, 0.17)
    image = tfa.image.rotate(image, angle, fill_mode='nearest')
    
    zoom = tf.random.uniform([], 0.8, 1.2)
    h, w, _ = image.shape
    new_h = tf.cast(h * zoom, tf.int32)
    new_w = tf.cast(w * zoom, tf.int32)
    image = tf.image.resize(image, (new_h, new_w))
    image = tf.image.resize_with_crop_or_pad(image, h, w)
    
    image = tf.image.random_brightness(image, max_delta=0.3)
    image = tf.image.random_contrast(image, lower=0.7, upper=1.3)
    image = tf.image.random_flip_left_right(image)
    image = tf.clip_by_value(image, 0.0, 1.0)
    
    return image, label

train_dataset = train_dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)

if not ENSURE_DETERMINISM:
    train_dataset = train_dataset.shuffle(buffer_size=BATCH_SIZE*8)

train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=False)
train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)
validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=False)


model = Sequential(name='FaceSentiment_FastSeparable')

# Block 1: Initial feature extraction
# First conv stays standard (better for raw input)
model.add(Conv2D(
    32,
    kernel_size=3,
    strides=1,
    padding='same',
    activation='relu',
    kernel_regularizer=l2(0.0005),
    input_shape=(48, 48, 1),
    name='conv1'
))
model.add(BatchNormalization(name='bn1'))

# Second conv â†’ SeparableConv2D (SPEEDUP)
model.add(SeparableConv2D(
    32,
    kernel_size=3,
    padding='same',
    activation='relu',
    depthwise_regularizer=l2(0.0005),
    pointwise_regularizer=l2(0.0005),
    name='sepconv1b'
))
model.add(BatchNormalization(name='bn1b'))
model.add(MaxPooling2D(pool_size=2, strides=2, name='pool1'))  # 24x24
model.add(Dropout(0.3, name='dropout1'))

# Block 2: Pattern learning with SeparableConv2D
model.add(SeparableConv2D(
    64,
    kernel_size=3,
    padding='same',
    activation='relu',
    depthwise_regularizer=l2(0.0005),
    pointwise_regularizer=l2(0.0005),
    name='sepconv2'
))
model.add(BatchNormalization(name='bn2'))

model.add(SeparableConv2D(
    64,
    kernel_size=3,
    padding='same',
    activation='relu',
    depthwise_regularizer=l2(0.0005),
    pointwise_regularizer=l2(0.0005),
    name='sepconv2b'
))
model.add(BatchNormalization(name='bn2b'))
model.add(MaxPooling2D(pool_size=2, strides=2, name='pool2'))  # 12x12
model.add(Dropout(0.4, name='dropout2'))

# Block 3: Deep feature extraction with SeparableConv2D
model.add(SeparableConv2D(
    96,
    kernel_size=3,
    padding='same',
    activation='relu',
    depthwise_regularizer=l2(0.001),
    pointwise_regularizer=l2(0.001),
    name='sepconv3'
))
model.add(BatchNormalization(name='bn3'))

model.add(SeparableConv2D(
    96,
    kernel_size=3,
    padding='same',
    activation='relu',
    depthwise_regularizer=l2(0.001),
    pointwise_regularizer=l2(0.001),
    name='sepconv3b'
))
model.add(BatchNormalization(name='bn3b'))
model.add(MaxPooling2D(pool_size=2, strides=2, name='pool3'))  # 6x6
model.add(Dropout(0.5, name='dropout3'))

# Block 4: Final refinement with SeparableConv2D
model.add(SeparableConv2D(
    128,
    kernel_size=3,
    padding='same',
    activation='relu',
    depthwise_regularizer=l2(0.0015),
    pointwise_regularizer=l2(0.0015),
    name='sepconv4'
))
model.add(BatchNormalization(name='bn4'))
model.add(Dropout(0.5, name='dropout4'))

# Classifier head (unchanged)
model.add(GlobalAveragePooling2D(name='gap'))
model.add(Dropout(0.6, name='dropout_gap'))

model.add(Dense(
    64,
    activation='relu',
    kernel_regularizer=l2(0.0015),
    name='fc1'
))
model.add(Dropout(0.6, name='dropout_fc'))

model.add(Dense(
    classes,
    activation='softmax',
    kernel_regularizer=l2(0.0015),
    name='predictions'
))

model.summary()

# Optimizer (unchanged)
opt = Adam(
    learning_rate=LEARNING_RATE,
    beta_1=0.9,
    beta_2=0.999,
    decay=1e-6
)

callbacks.append(BatchLoggerCallback(
    BATCH_SIZE, 
    train_sample_count, 
    epochs=EPOCHS, 
    ensure_determinism=ENSURE_DETERMINISM
))

callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=8,
    min_lr=1e-7,
    verbose=1,
    mode='min'
))

callbacks.append(tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=40,
    restore_best_weights=True,
    verbose=1,
    mode='min'
))

callbacks.append(tf.keras.callbacks.ModelCheckpoint(
    'best_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
))

# Compile
model.compile(
    loss='categorical_crossentropy', 
    optimizer=opt, 
    metrics=['accuracy']
)

# Class weights (unchanged)
class_weights = ei_tensorflow.training.get_class_weights(Y_train)
dampened_weights = {k: 1 + 0.3*(v-1) for k, v in class_weights.items()}

# Train
model.fit(
    train_dataset, 
    epochs=EPOCHS, 
    validation_data=validation_dataset, 
    verbose=2, 
    callbacks=callbacks, 
    class_weight=dampened_weights
)

disable_per_channel_quantization = False