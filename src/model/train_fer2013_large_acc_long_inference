import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Dense, Dropout, Conv2D, MaxPooling2D, 
                                      BatchNormalization, GlobalAveragePooling2D)
from tensorflow.keras.optimizers.legacy import Adam
from tensorflow.keras.regularizers import l2

# Training parameters
EPOCHS = args.epochs or 160
LEARNING_RATE = args.learning_rate or 0.0005  # Lower LR for stability
BATCH_SIZE = args.batch_size or 32
ENSURE_DETERMINISM = args.ensure_determinism

# Enhanced data augmentation
def augment_image(image, label):
    # Random rotation
    angle = tf.random.uniform([], -0.17, 0.17)  # ~Â±10 degrees
    image = tfa.image.rotate(image, angle, fill_mode='nearest')

    # Random zoom
    zoom = tf.random.uniform([], 0.8, 1.2)
    h, w, _ = image.shape
    new_h = tf.cast(h * zoom, tf.int32)
    new_w = tf.cast(w * zoom, tf.int32)
    image = tf.image.resize(image, (new_h, new_w))
    image = tf.image.resize_with_crop_or_pad(image, h, w)

    # Random brightness
    image = tf.image.random_brightness(image, max_delta=0.3)
    # Random contrast
    image = tf.image.random_contrast(image, lower=0.7, upper=1.3)
    # Random flip (horizontal only)
    image = tf.image.random_flip_left_right(image)
    # Clip to valid range
    image = tf.clip_by_value(image, 0.0, 1.0)

    return image, label

# Apply augmentation to training data
train_dataset = train_dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)

if not ENSURE_DETERMINISM:
    train_dataset = train_dataset.shuffle(buffer_size=BATCH_SIZE*8)

train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=False)
train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)
validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=False)

# Model Architecture - Balanced regularization
model = Sequential(name='FaceSentiment_Regularized')

# Block 1: Initial feature extraction
model.add(Conv2D(
    32,
    kernel_size=3,
    strides=1,
    padding='same',
    activation='relu',
    kernel_regularizer=l2(0.0005),  # Light L2
    input_shape=(48, 48, 1),
    name='conv1'
))
model.add(BatchNormalization(name='bn1'))
model.add(Conv2D(
    32,
    kernel_size=3,
    padding='same',
    activation='relu',
    kernel_regularizer=l2(0.0005),
    name='conv1b'
))
model.add(BatchNormalization(name='bn1b'))
model.add(MaxPooling2D(pool_size=2, strides=2, name='pool1'))  # 24x24
model.add(Dropout(0.3, name='dropout1'))  # Moderate dropout early

# Block 2: Pattern learning
model.add(Conv2D(
    64,
    kernel_size=3,
    padding='same',
    activation='relu',
    kernel_regularizer=l2(0.0005),
    name='conv2'
))
model.add(BatchNormalization(name='bn2'))
model.add(Conv2D(
    64,
    kernel_size=3,
    padding='same',
    activation='relu',
    kernel_regularizer=l2(0.0005),
    name='conv2b'
))
model.add(BatchNormalization(name='bn2b'))
model.add(MaxPooling2D(pool_size=2, strides=2, name='pool2'))  # 12x12
model.add(Dropout(0.4, name='dropout2'))  # Increasing dropout

# Block 3: Deep feature extraction
model.add(Conv2D(
    96,  # Reduced from 128 to limit capacity
    kernel_size=3,
    padding='same',
    activation='relu',
    kernel_regularizer=l2(0.001),  # Stronger regularization in deep layers
    name='conv3'
))
model.add(BatchNormalization(name='bn3'))
model.add(Conv2D(
    96,
    kernel_size=3,
    padding='same',
    activation='relu',
    kernel_regularizer=l2(0.001),
    name='conv3b'
))
model.add(BatchNormalization(name='bn3b'))
model.add(MaxPooling2D(pool_size=2, strides=2, name='pool3'))  # 6x6
model.add(Dropout(0.5, name='dropout3'))  # Heavy dropout in deep layers

# Block 4: Final refinement (SMALLER to reduce overfitting)
model.add(Conv2D(
    128,  # Reduced from 256
    kernel_size=3,
    padding='same',
    activation='relu',
    kernel_regularizer=l2(0.0015),  # Strongest L2 here
    name='conv4'
))
model.add(BatchNormalization(name='bn4'))
model.add(Dropout(0.5, name='dropout4'))

# Classifier head with aggressive regularization
model.add(GlobalAveragePooling2D(name='gap'))
model.add(Dropout(0.6, name='dropout_gap'))  # Heavy dropout before dense

model.add(Dense(
    64,  # Reduced from 128 - smaller bottleneck
    activation='relu',
    kernel_regularizer=l2(0.0015),  # Strong L2 on dense layer
    name='fc1'
))
model.add(Dropout(0.6, name='dropout_fc'))

model.add(Dense(
    classes,
    activation='softmax',
    kernel_regularizer=l2(0.0015),
    name='predictions'
))

model.summary()

# Optimizer with weight decay
opt = Adam(
    learning_rate=LEARNING_RATE,
    beta_1=0.9,
    beta_2=0.999,
    decay=1e-6  # Additional weight decay
)

# Callbacks with stricter early stopping
callbacks.append(BatchLoggerCallback(
    BATCH_SIZE, 
    train_sample_count, 
    epochs=EPOCHS, 
    ensure_determinism=ENSURE_DETERMINISM
))

# Learning rate reduction - more aggressive
callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',  # Monitor loss, not accuracy
    factor=0.5,
    patience=8,
    min_lr=1e-7,
    verbose=1,
    mode='min'
))

# Early stopping - prevent overtraining
callbacks.append(tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=40,
    restore_best_weights=True,
    verbose=1,
    mode='min'
))

# OPTIONAL: Model checkpoint to save best validation model
callbacks.append(tf.keras.callbacks.ModelCheckpoint(
    'best_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
))

# Compile
model.compile(
    loss='categorical_crossentropy', 
    optimizer=opt, 
    metrics=['accuracy']
)

# Moderate class weights to avoid overfitting to minority classes
class_weights = ei_tensorflow.training.get_class_weights(Y_train)
# Dampen weights to reduce overfitting
dampened_weights = {k: 1 + 0.3*(v-1) for k, v in class_weights.items()}

# Train
model.fit(
    train_dataset, 
    epochs=EPOCHS, 
    validation_data=validation_dataset, 
    verbose=2, 
    callbacks=callbacks, 
    class_weight=dampened_weights
)